from typing import List, Dict
import json
import os
import uuid
from dataclasses import dataclass
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent
from langgraph.graph.state import CompiledStateGraph
from dotenv import load_dotenv
load_dotenv()

# API Key
API_KEY = os.getenv("SILICONFLOW_API_KEY")

@dataclass
class FileInfo:
    """æ–‡ä»¶ä¿¡æ¯"""
    id: int
    filename: str
    chunk_count: int
    status: str = "done"

@dataclass
class FileChunk:
    """æ–‡ä»¶ç‰‡æ®µ"""
    file_id: int
    chunk_index: int
    content: str


class KnowledgeBaseManager:
    def __init__(self):
        # æ¨¡æ‹Ÿæ–‡æ¡£å†…å®¹
        self.files = [
            FileInfo(id=1, filename="rag_introduction.md", chunk_count=5),
            FileInfo(id=2, filename="llm_fundamentals.md", chunk_count=4),
            FileInfo(id=3, filename="vector_search.md", chunk_count=3),
            FileInfo(id=4, filename="prompt_engineering.md", chunk_count=4),
        ]

        self.chunks = {
            (1, 0): FileChunk(
                file_id=1,
                chunk_index=0,
                content="RAG (Retrieval-Augmented Generation) æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„æŠ€æœ¯ï¼Œé€šè¿‡ä»å¤–éƒ¨çŸ¥è¯†æºæ£€ç´¢ç›¸å…³ä¿¡æ¯æ¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚"
            ),
            (1, 1): FileChunk(
                file_id=1,
                chunk_index=1,
                content="RAG çš„ä¼˜ç‚¹åŒ…æ‹¬ï¼š1) èƒ½å¤Ÿè®¿é—®æœ€æ–°ä¿¡æ¯ï¼Œ2) å‡å°‘æ¨¡å‹å¹»è§‰ï¼Œ3) æä¾›å¯è¿½æº¯çš„ä¿¡æ¯æ¥æºï¼Œ4) æ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹å³å¯æ›´æ–°çŸ¥è¯†ã€‚"
            ),
            (1, 2): FileChunk(
                file_id=1,
                chunk_index=2,
                content="RAG çš„ç¼ºç‚¹åŒ…æ‹¬ï¼š1) æ£€ç´¢è´¨é‡ç›´æ¥å½±å“ç”Ÿæˆæ•ˆæœï¼Œ2) å¢åŠ äº†ç³»ç»Ÿå¤æ‚åº¦ï¼Œ3) å¯¹å‘é‡æ•°æ®åº“çš„ä¾èµ–ï¼Œ4) å¯èƒ½å­˜åœ¨æ£€ç´¢å»¶è¿Ÿã€‚"
            ),
            (1, 3): FileChunk(
                file_id=1,
                chunk_index=3,
                content="ä¼ ç»Ÿ RAG ç³»ç»Ÿé€šå¸¸é‡‡ç”¨å›ºå®šçš„æ£€ç´¢-ç”Ÿæˆæµç¨‹ï¼Œæ— æ³•æ ¹æ®é—®é¢˜å¤æ‚åº¦åŠ¨æ€è°ƒæ•´ç­–ç•¥ã€‚"
            ),
            (1, 4): FileChunk(
                file_id=1,
                chunk_index=4,
                content="Agentic RAG é€šè¿‡å¼•å…¥æ™ºèƒ½ä½“ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿè‡ªä¸»å†³ç­–ä½•æ—¶æ£€ç´¢ã€å¦‚ä½•æ£€ç´¢ä»¥åŠæ£€ç´¢å¤šå°‘å†…å®¹ï¼Œä»è€Œæå‡å¤æ‚é—®é¢˜çš„å¤„ç†èƒ½åŠ›ã€‚"
            ),
            (2, 0): FileChunk(
                file_id=2,
                chunk_index=0,
                content="å¤§è¯­è¨€æ¨¡å‹ (LLM) æ˜¯åŸºäº Transformer æ¶æ„çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œé€šè¿‡é¢„è®­ç»ƒå­¦ä¹ è¯­è¨€çš„ç»Ÿè®¡è§„å¾‹ã€‚"
            ),
            (2, 1): FileChunk(
                file_id=2,
                chunk_index=1,
                content="LLM çš„æ ¸å¿ƒèƒ½åŠ›åŒ…æ‹¬è‡ªç„¶è¯­è¨€ç†è§£ã€ç”Ÿæˆã€æ¨ç†å’Œå°‘æ ·æœ¬å­¦ä¹ ç­‰ã€‚"
            ),
            (2, 2): FileChunk(
                file_id=2,
                chunk_index=2,
                content="LLM çš„å±€é™æ€§åŒ…æ‹¬çŸ¥è¯†æˆªæ­¢æ—¶é—´ã€å¯èƒ½äº§ç”Ÿå¹»è§‰ã€è®¡ç®—èµ„æºæ¶ˆè€—å¤§ç­‰ã€‚"
            ),
            (2, 3): FileChunk(
                file_id=2,
                chunk_index=3,
                content="å·¥å…·è°ƒç”¨æ˜¯ LLM çš„é‡è¦æ‰©å±•èƒ½åŠ›ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä¸å¤–éƒ¨ç³»ç»Ÿäº¤äº’ï¼Œæ‰§è¡Œå¤æ‚ä»»åŠ¡ã€‚"
            ),
            (3, 0): FileChunk(
                file_id=3,
                chunk_index=0,
                content="å‘é‡æœç´¢æ˜¯ RAG ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œé€šè¿‡å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºæ¥å®ç°è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…ã€‚"
            ),
            (3, 1): FileChunk(
                file_id=3,
                chunk_index=1,
                content="å¸¸è§çš„å‘é‡æœç´¢ç®—æ³•åŒ…æ‹¬ FAISSã€Chromaã€Pinecone ç­‰ï¼Œå„æœ‰ä¸åŒçš„æ€§èƒ½ç‰¹ç‚¹ã€‚"
            ),
            (3, 2): FileChunk(
                file_id=3,
                chunk_index=2,
                content="å‘é‡æœç´¢çš„æ•ˆæœå¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºembeddingæ¨¡å‹çš„è´¨é‡å’Œç´¢å¼•æ„å»ºç­–ç•¥ã€‚"
            ),
            (4, 0): FileChunk(
                file_id=4,
                chunk_index=0,
                content="æç¤ºå·¥ç¨‹æ˜¯ä¼˜åŒ–å¤§æ¨¡å‹è¡¨ç°çš„é‡è¦æŠ€æœ¯ï¼ŒåŒ…æ‹¬è®¾è®¡æœ‰æ•ˆçš„æç¤ºæ¨¡æ¿ã€ä¸Šä¸‹æ–‡ç®¡ç†ç­‰ã€‚"
            ),
            (4, 1): FileChunk(
                file_id=4,
                chunk_index=1,
                content="è‰¯å¥½çš„æç¤ºè®¾è®¡åŸåˆ™åŒ…æ‹¬ï¼šæ¸…æ™°æ˜ç¡®ã€æä¾›ç¤ºä¾‹ã€ç»“æ„åŒ–è¾“å‡ºæ ¼å¼ç­‰ã€‚"
            ),
            (4, 2): FileChunk(
                file_id=4,
                chunk_index=2,
                content="Agent ç³»ç»Ÿçš„æç¤ºè®¾è®¡éœ€è¦è€ƒè™‘å·¥å…·è°ƒç”¨çš„ç­–ç•¥æŒ‡å¯¼å’Œé”™è¯¯å¤„ç†æœºåˆ¶ã€‚"
            ),
            (4, 3): FileChunk(
                file_id=4,
                chunk_index=3,
                content="ç³»ç»Ÿæç¤ºè¯åº”è¯¥æ˜ç¡®å®šä¹‰ Agent çš„è§’è‰²ã€èƒ½åŠ›è¾¹ç•Œå’Œè¡Œä¸ºè§„èŒƒã€‚"
            ),
        }

    def search(self, kb_id: int, query: str) -> List[Dict]:
        """åŸºäºå…³é”®è¯åŒ¹é…"""
        query_lower = query.lower()
        keywords = ["rag", "agentic", "ä¼˜ç¼ºç‚¹", "ä¼˜ç‚¹", "ç¼ºç‚¹", "llm", "æ£€ç´¢", "ç”Ÿæˆ", "å‘é‡", "æœç´¢"]
        results = []

        # å…³é”®è¯è¯„åˆ†
        for (file_id, chunk_idx), chunk in self.chunks.items():
            content_lower = chunk.content.lower()
            score = 0
            
            for keyword in keywords:
                if keyword in query_lower and keyword in content_lower:
                    score += 1

            if score > 0 or any(word in content_lower for word in query_lower.split()):
                file_info = next(f for f in self.files if f.id == file_id)
                results.append(
                    {
                        "file_id": file_id,
                        "chunk_index": chunk_idx,
                        "filename": file_info.filename,
                        "score": score + 0.5,
                        "preview": chunk.content[:100] + "..." if len(chunk.content) > 100 else chunk.content,
                    }
                )

        # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›å‰5ä¸ª
        results.sort(key=lambda x: x["score"], reverse=True)
        return results[:5]

    def getFilesMeta(self, kb_id: int, file_ids: List[int]) -> List[Dict]:
        """è·å–æ–‡ä»¶å…ƒä¿¡æ¯"""
        result = []
        for file_id in file_ids:
            file_info = next((f for f in self.files if f.id == file_id), None)
            if file_info:
                result.append(
                    {
                        "id": file_info.id,
                        "filename": file_info.filename,
                        "chunk_count": file_info.chunk_count,
                        "status": file_info.status,
                    }
                )
        return result

    def readFileChunks(self, kb_id: int, chunks: List[Dict[str, int]]) -> List[Dict]:
        """è¯»å–å…·ä½“çš„æ–‡ä»¶ç‰‡æ®µ"""
        result = []
        for chunk_spec in chunks:
            file_id = chunk_spec.get("fileId")
            chunk_index = chunk_spec.get("chunkIndex")

            chunk = self.chunks.get((file_id, chunk_index))
            if chunk:
                result.append(
                    {
                        "file_id": file_id,
                        "chunk_index": chunk_index,
                        "content": chunk.content,
                        "filename": next(f.filename for f in self.files if f.id == file_id),
                    }
                )
        return result

    def listFilesPaginated(self, kb_id: int, page: int, page_size: int) -> List[Dict]:
        """åˆ†é¡µåˆ—å‡ºæ–‡ä»¶"""
        start = page * page_size
        end = start + page_size

        files_slice = self.files[start:end]
        return [
            {
                "id": f.id,
                "filename": f.filename,
                "chunk_count": f.chunk_count,
                "status": f.status,
            }
            for f in files_slice
        ]


# åˆå§‹åŒ–æ¨¡æ‹Ÿçš„çŸ¥è¯†åº“æ§åˆ¶å™¨
kb_controller = KnowledgeBaseManager()
knowledge_base_id = 1  # æ¨¡æ‹Ÿçš„çŸ¥è¯†åº“ID

# å®šä¹‰å››ä¸ªæ ¸å¿ƒå·¥å…·
@tool("query_knowledge_base")
def query_knowledge_base(query: str) -> str:
    """
    é€šè¿‡å…³é”®è¯åŒ¹é…æœç´¢çŸ¥è¯†åº“ä¸­çš„ç›¸å…³å†…å®¹
    
    å‚æ•°:
        query: str - æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œå°†ç”¨äºåŒ¹é…çŸ¥è¯†åº“ä¸­çš„å†…å®¹ç‰‡æ®µ
    
    è¿”å›:
        str - JSONæ ¼å¼çš„æœç´¢ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœåŒ…å«ä»¥ä¸‹å­—æ®µ:
            - file_id: int - æ–‡ä»¶ID
            - chunk_index: int - å†…å®¹ç‰‡æ®µç´¢å¼•
            - filename: str - æ–‡ä»¶å
            - score: float - åŒ¹é…åˆ†æ•°
            - preview: str - å†…å®¹é¢„è§ˆï¼ˆæœ€å¤š100ä¸ªå­—ç¬¦ï¼‰
    """
    results = kb_controller.search(knowledge_base_id, query)
    return json.dumps(results, ensure_ascii=False, indent=2)


@tool("get_files_meta")
def get_files_meta(fileIds: List[int]) -> str:
    """
    è·å–çŸ¥è¯†åº“ä¸­æŒ‡å®šæ–‡ä»¶çš„å…ƒä¿¡æ¯
    
    å‚æ•°:
        fileIds: List[int] - æ–‡ä»¶IDåˆ—è¡¨ï¼Œç”¨äºæŒ‡å®šè¦è·å–å…ƒä¿¡æ¯çš„æ–‡ä»¶
    
    è¿”å›:
        str - JSONæ ¼å¼çš„æ–‡ä»¶å…ƒä¿¡æ¯åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡ä»¶ä¿¡æ¯åŒ…å«ä»¥ä¸‹å­—æ®µ:
            - id: int - æ–‡ä»¶ID
            - filename: str - æ–‡ä»¶å
            - chunk_count: int - æ–‡ä»¶åŒ…å«çš„å†…å®¹ç‰‡æ®µæ•°é‡
            - status: str - æ–‡ä»¶çŠ¶æ€
        å¦‚æœæœªæä¾›fileIdsï¼Œè¿”å›æç¤ºä¿¡æ¯"è¯·æä¾›æ–‡ä»¶IDæ•°ç»„"
    """
    if not fileIds:
        return "è¯·æä¾›æ–‡ä»¶IDæ•°ç»„"
    results = kb_controller.getFilesMeta(knowledge_base_id, fileIds)
    return json.dumps(results, ensure_ascii=False, indent=2)


@tool("read_file_chunks")
def read_file_chunks(chunks: List[Dict[str, int]]) -> str:
    """
    è¯»å–çŸ¥è¯†åº“ä¸­æŒ‡å®šæ–‡ä»¶çš„å†…å®¹ç‰‡æ®µ
    
    å‚æ•°:
        chunks: List[Dict[str, int]] - è¦è¯»å–çš„å†…å®¹ç‰‡æ®µåˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸éœ€åŒ…å«:
            - fileId: int - æ–‡ä»¶ID
            - chunkIndex: int - å†…å®¹ç‰‡æ®µç´¢å¼•
    
    è¿”å›:
        str - JSONæ ¼å¼çš„å†…å®¹ç‰‡æ®µåˆ—è¡¨ï¼Œæ¯ä¸ªç‰‡æ®µåŒ…å«ä»¥ä¸‹å­—æ®µ:
            - file_id: int - æ–‡ä»¶ID
            - chunk_index: int - å†…å®¹ç‰‡æ®µç´¢å¼•
            - content: str - å®Œæ•´çš„å†…å®¹ç‰‡æ®µæ–‡æœ¬
            - filename: str - å…³è”çš„æ–‡ä»¶å
        å¦‚æœæœªæä¾›chunksï¼Œè¿”å›æç¤ºä¿¡æ¯"è¯·æä¾›è¦è¯»å–çš„chunkä¿¡æ¯æ•°ç»„"
    """
    if not chunks:
        return "è¯·æä¾›è¦è¯»å–çš„chunkä¿¡æ¯æ•°ç»„"
    results = kb_controller.readFileChunks(knowledge_base_id, chunks)
    return json.dumps(results, ensure_ascii=False, indent=2)


@tool("list_files")
def list_files(page: int = 0, pageSize: int = 10) -> str:
    """
    åˆ†é¡µåˆ—å‡ºçŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰æ–‡ä»¶
    
    å‚æ•°:
        page: int - é¡µç ï¼Œä»0å¼€å§‹ï¼ˆé»˜è®¤ä¸º0ï¼‰
        pageSize: int - æ¯é¡µæ˜¾ç¤ºçš„æ–‡ä»¶æ•°é‡ï¼ˆé»˜è®¤ä¸º10ï¼‰
    
    è¿”å›:
        str - JSONæ ¼å¼çš„æ–‡ä»¶åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡ä»¶ä¿¡æ¯åŒ…å«ä»¥ä¸‹å­—æ®µ:
            - id: int - æ–‡ä»¶ID
            - filename: str - æ–‡ä»¶å
            - chunk_count: int - æ–‡ä»¶åŒ…å«çš„å†…å®¹ç‰‡æ®µæ•°é‡
            - status: str - æ–‡ä»¶çŠ¶æ€
    """
    results = kb_controller.listFilesPaginated(knowledge_base_id, page, pageSize)
    return json.dumps(results, ensure_ascii=False, indent=2)


# ç³»ç»Ÿæç¤ºè¯
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ª Agentic RAG åŠ©æ‰‹ã€‚è¯·éµå¾ªä»¥ä¸‹ç­–ç•¥é€æ­¥æ”¶é›†è¯æ®åå›ç­”ï¼š

1. å…ˆç”¨ query_knowledge_base æœç´¢ç›¸å…³å†…å®¹ï¼Œè·å¾—å€™é€‰æ–‡ä»¶å’Œç‰‡æ®µçº¿ç´¢
2. æ ¹æ®æœç´¢ç»“æœï¼Œé€‰æ‹©æœ€ç›¸å…³çš„æ–‡ä»¶ï¼Œå¯é€‰æ‹©æ€§ä½¿ç”¨ get_files_meta æŸ¥çœ‹è¯¦ç»†æ–‡ä»¶ä¿¡æ¯
3. ä½¿ç”¨ read_file_chunks ç²¾è¯»æœ€ç›¸å…³çš„2-3ä¸ªç‰‡æ®µå†…å®¹ä½œä¸ºè¯æ®
4. åŸºäºè¯»å–çš„å…·ä½“ç‰‡æ®µå†…å®¹ç»„ç»‡ç­”æ¡ˆ
5. å›ç­”æœ«å°¾ç”¨"å¼•ç”¨ï¼š"æ ¼å¼åˆ—å‡ºå®é™…è¯»å–çš„fileIdå’ŒchunkIndex

é‡è¦åŸåˆ™ï¼š
- ä¸è¦ç¼–é€ ä¿¡æ¯ï¼ŒåªåŸºäºå®é™…è¯»å–çš„ç‰‡æ®µå†…å®¹å›ç­”
- è‹¥è¯æ®ä¸è¶³ï¼Œè¯·è¯´æ˜å¹¶å»ºè®®è¿›ä¸€æ­¥æœç´¢çš„æ–¹å‘
- ä¼˜å…ˆé€‰æ‹©è¯„åˆ†é«˜çš„æœç´¢ç»“æœè¿›è¡Œæ·±å…¥é˜…è¯»
"""

def create_agentic_rag_system():
    """åˆ›å»º Agentic RAG ç³»ç»Ÿ"""
    # å·¥å…·æ¸…å•
    tools = [query_knowledge_base, get_files_meta, read_file_chunks, list_files]

    # æ¨¡å‹ä¸ Agent
    llm = ChatOpenAI(
        temperature=0,
        max_retries=3,
        base_url="https://api.siliconflow.cn/v1",
        model="deepseek-ai/DeepSeek-V3",
        api_key=API_KEY,
    )
    agent = create_react_agent(llm, tools, prompt=SYSTEM_PROMPT)
    return agent

def run_agent(agent: CompiledStateGraph, messages: list): 
    thread_id = str(uuid.uuid4())  # ç”Ÿæˆå”¯ä¸€çš„çº¿ç¨‹ID
    result = agent.stream( 
        {"messages": messages}, 
        stream_mode="values", 
        config={"thread_id": thread_id}, 
    ) 
    for chunk in result: 
        message = chunk["messages"] 
        last_message = message[-1] 
        last_message.pretty_print() 
    return message

def main():
    agent = create_agentic_rag_system()
    # æµ‹è¯•é—®é¢˜
    question = "è¯·åŸºäºçŸ¥è¯†åº“ï¼Œæ¦‚è¿° RAG çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶ç»™å‡ºå¼•ç”¨ã€‚"
    print(f"\nâ“ é—®é¢˜: {question}")
    print("\nğŸ¤” Agent æ€è€ƒä¸è¡ŒåŠ¨è¿‡ç¨‹:")
    print("-" * 50)

    run_agent(agent, [("user", question)])

if __name__ == "__main__":
    main()